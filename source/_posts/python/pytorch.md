---
title: pytorch学习
date: 2024-11-1
---



# nn模块



## 权重初始化

```python
import torch
import torch.nn as nn


# 定义线性层
linear_layer = nn.Linear(in_features=10, out_features=5)
# 直接赋值给权重和偏置
linear_layer.weight.data.fill_(0.01)  # 将权重全设置为 0.01
linear_layer.bias.data.fill_(0)       # 将偏置设置为 0
```



# 广播机制

广播机制的核心流程可以概括为：

1. **从右往左逐个对比两个张量的维度**。
2. **相等时继续**：如果对应维度大小相等，直接匹配。
3. **不相等时判断是否有1**：如果不相等，检查其中一个维度是否为1。如果是，则可以广播扩展成相等的维度。
4. **无法匹配时停止**：如果对应维度大小不相等，且都不为1，则广播机制无法继续，运算会报错。
5. **缺少的维度补1**：如果一个张量维度数量少，从右侧开始自动补充1，以便进行对比。

例如，如果有两个张量：

- `a` 的形状是 `(3, 1, 5)`
- `b` 的形状是 `(4, 5)`

按照广播机制，从右往左依次匹配：

- 最右边的维度 5 匹配。
- 中间的维度 `1`（`a` 的第二维）可以扩展到 4，以匹配 `b` 的第二维。
- `a` 的第一个维度 `3` 与 `b` 无需匹配，因为 `b` 没有对应的维度，所以直接保留。

最终 `a` 会广播为 `(3, 4, 5)`，`b` 会广播为 `(1, 4, 5)`，最后结果为 `(3, 4, 5)`。

> pytorch并不会真正的扩展原始数据维度, 而是在计算的时候重复利用原始数据

**注意**: 矩阵乘法(点积) 有自己的运算规则, 并不满足这种广播机制

# 技巧

## 损失函数数值图

我们在训练模型的时候, 推荐把每一轮的损失值画出来, 因为深度学习是个黑盒子, 我们很难得知他到底学啥了, 学的咋样了, 我们只有通过我们自己的评判标准也就是损失去判断, 所以把图画出来是最直观的, 也方便调试bug



# 问题

## 梯度爆炸

在使用MSELoss的时候, 如果reduction改成sum,  那么损失值跟batch就线性相关了, 很容易导致梯度爆炸,  就需要对这个reduction做调整, 要么使用mean, 要么自己去定义



# GPU内存

