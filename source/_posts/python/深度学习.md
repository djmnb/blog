---
title: 深度学习
date: 2024-9-30
---

> [学习链接](zh.d2l.a)

# 前馈神经网络(FNN)

即多层感知机(MLP),  输入层 与 隐藏层 之间, 以及隐藏层和输出层之间 采用全连接的方式

# 卷积神经网络(CNN)

这个给我的感觉就是用来做特征提取,  使用一小块的卷积核 依次去以某种模式去匹配一个矩阵(图像, 以及其他能被矩阵表达的东西), 然后得出匹配的特征, 然后做池化, 如此往复, 最后再使用前馈神经网络输出东西

* 卷积核: 用来做特征匹配的, 也是需要学习的参数, 卷积核的通道数与输入数据的通道数是一样的.  一个卷积核就是一种特征提取
* 池化: 降低计算复杂度, 减少过拟合,保留我们需要的信息, 只会改变宽高, 不会改变通道数
* 感受野: 能看到原始输入数据区域大小

# 循环神经网络(RNN)

前馈神经网络是没有记忆能力的, 不能根据之前的输入做出不同的输出, 只要是相同的东西都是同样的输出, 而循环神经网络是有记忆能力的, 他能理解上下文

# 注意力机制

他是一种模仿人类注意力的机制，能够让模型在处理复杂任务时更关注某些关键的信息或部分

举个例子:  你手里有很多同事的笔记（键），你自己有一个具体的问题（查询），你要从这些笔记中找到和你问题最相关的信息（值）。你可能会在不同的同事那里找到不同程度的帮助，所以你会根据他们的经验来判断谁的笔记最有价值（注意力权重）。最后，你整合了最有帮助的内容，得到了答案。

* **查询**是你当前想要解决的问题。

* **键**是整个上下文背景，告诉你每个部分的信息如何相互关联。

* **值**是你从键中提取的答案或信息。

* **注意力权重**决定了你应该关注哪些信息，并忽略哪些不相关的部分。

我们的模型就是需要训练出如何根据输入得到查询, 键, 值,  根据输入得到关注的点(查询), 得到 不同部分(键), 不同部分中的关键信息(值),  然后计算查询与键的注意力权重, 然后将权重与值加权得到输出(整合)



# 额外笔记



* 权重衰退: 其实就是正则化, 让复杂的权重产生更大的损失, 这样可以避免过拟合

* 丢弃法: 也就是dropout,  通过丢弃一些节点(训练过程中), 相当于加入一些噪音, 提高模型的鲁棒性和泛化能力, **也是相当于正则化**,  p的概率丢弃这个节点输出, 1-p的概率将输出除以1-p 其实这样的话会发现这一层的输出均值不变

* 数值稳定性: 训练过程中可能出现梯度爆炸或者梯度消失, 这样都会对模型的训练产生问题, 我们希望模型的训练比较稳定, 一般通过控制反向传播和前馈传播的时候, 让输出的大小满足均值为0, 方差为一个固定值,  这可以通过权重初始化(一般是正态分布)来做到(但是也仅限于前期, 因为后期权重会发生变化, 可能就偏离了原始分布)

* batchNorm:  用于固定小批量里面的均值和方差, 可以加速模型收敛速度, 但是一般不会改变模型精度, 相比权重初始化, 这个东西可以在全局爆炸数据满足某种分布, 而权重初始化只能在开始的时候保证满足某种分布

  

